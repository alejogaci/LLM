# Trend Micro AI Assistant - LLM con Interfaz Moderna

AplicaciÃ³n de chat con IA usando modelos libres (Llama, Mistral) con interfaz web sofisticada en colores de Trend Micro.

---

## ğŸš€ INICIO RÃPIDO (2 comandos)

```bash
# 1. Instalar (solo primera vez - tarda 10 min)
./setup.sh

# 2. Ejecutar (todo en segundo plano)
./run.sh
```

**Acceso:** `http://localhost:5000` (o `http://IP_PUBLICA:5000` en AWS)

---

## ğŸ“‹ COMANDOS PRINCIPALES

| Comando | QuÃ© hace |
|---------|----------|
| `./setup.sh` | Instala Ollama, modelo IA, dependencias (solo 1 vez) |
| `./run.sh` | Inicia TODO en segundo plano |
| `./stop.sh` | Detiene TODO |
| `./status.sh` | Muestra estado de servicios y logs |

---

## ğŸ“‚ ESTRUCTURA DE ARCHIVOS

```
trend-ai-assistant/
â”œâ”€â”€ app.py                 # Backend Flask
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ README.md              # Esta guÃ­a
â”‚
â”œâ”€â”€ setup.sh               # â­ InstalaciÃ³n
â”œâ”€â”€ run.sh                 # â­ Ejecutar
â”œâ”€â”€ stop.sh                # â­ Detener
â”œâ”€â”€ status.sh              # â­ Ver estado
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Interfaz web
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css      # Estilos Trend Micro
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ script.js      # JavaScript
â”‚
â””â”€â”€ logs/                  # Se crea automÃ¡ticamente
    â”œâ”€â”€ app.log
    â””â”€â”€ ollama.log
```

---

## âœ¨ CARACTERÃSTICAS

- ğŸ¨ **Interfaz moderna** con colores Trend Micro (rojos/oscuros)
- âš¡ **Streaming en tiempo real** (respuestas letra por letra)
- ğŸ¤– **Modelos libres** (Llama 3.2, Mistral, Phi-3)
- ğŸ”„ **Todo en segundo plano** (no necesitas mÃºltiples terminales)
- ğŸ“Š **Logs completos** guardados en archivos

---

## ğŸ–¥ï¸ REQUISITOS

- **Linux:** Ubuntu 22.04+ (recomendado)
- **Python:** 3.8 o superior
- **RAM:** 8 GB mÃ­nimo
- **Espacio:** ~5 GB

---

## â˜ï¸ DESPLIEGUE EN AWS

### Security Group:
```
Type: Custom TCP
Port: 5000
Source: 0.0.0.0/0 (o tu IP)
```

### Instancia recomendada:
```
AMI: Ubuntu Server 22.04 LTS
Tipo: t3.large (8 GB RAM)
Storage: 30 GB
```

### Pasos:
```bash
# 1. Conectar
ssh -i key.pem ubuntu@IP_PUBLICA

# 2. Subir archivos (desde tu PC)
scp -i key.pem -r trend-ai-assistant ubuntu@IP_PUBLICA:~/

# 3. En el servidor
cd trend-ai-assistant
chmod +x *.sh
./setup.sh
./run.sh

# 4. Acceder
# http://IP_PUBLICA:5000
```

---

## ğŸ”§ VER LOGS

```bash
# Ver estado general
./status.sh

# Logs en tiempo real
tail -f logs/app.log      # AplicaciÃ³n
tail -f logs/ollama.log   # IA

# Ãšltimas 50 lÃ­neas
tail -n 50 logs/app.log
```

---

## ğŸ› SOLUCIÃ“N DE PROBLEMAS

### Problema: No inicia
```bash
./status.sh              # Ver quÃ© estÃ¡ mal
cat logs/app.log         # Ver error especÃ­fico
./stop.sh && ./run.sh    # Reiniciar
```

### Problema: Puerto ocupado
```bash
kill -9 $(lsof -ti:5000)  # Liberar puerto
./run.sh
```

### Problema: Ollama no responde
```bash
pkill ollama              # Matar proceso
./run.sh                  # Reiniciar
```

---

## âš™ï¸ CONFIGURACIÃ“N

### Cambiar modelo:
Edita `app.py` lÃ­nea 9:
```python
MODEL = "llama3.2"  # Cambiar a "mistral", "phi3", etc.
```

### Cambiar puerto:
Edita `app.py` Ãºltima lÃ­nea:
```python
app.run(debug=False, host='0.0.0.0', port=5000)
```

### Instalar otros modelos:
```bash
ollama pull mistral
ollama pull phi3
ollama list  # Ver instalados
```

---

## ğŸ¯ FLUJO DE TRABAJO

### Primera instalaciÃ³n:
```bash
./setup.sh    # 10-15 minutos
./run.sh      # 5 segundos
```

### Uso diario:
```bash
./run.sh      # Iniciar
./stop.sh     # Detener
```

### Testing con GuardTrail:
1. Configura GuardTrail para monitorear puerto 5000
2. La app genera trÃ¡fico LLM real
3. Logs disponibles en `logs/`

---

## ğŸ“Š RECURSOS DEL SISTEMA

| Componente | RAM | CPU |
|-----------|-----|-----|
| Ollama + Llama 3.2 | ~3-4 GB | 10-50% |
| Flask App | ~50 MB | 1-5% |
| **Total** | **~4 GB** | **~15-55%** |

---

## ğŸ“ CHEAT SHEET

```bash
# InstalaciÃ³n
./setup.sh

# Control
./run.sh        # Iniciar
./stop.sh       # Detener
./status.sh     # Estado

# Logs
tail -f logs/app.log      # Ver logs app
tail -f logs/ollama.log   # Ver logs IA

# Reiniciar
./stop.sh && ./run.sh

# Verificar procesos
ps aux | grep ollama
ps aux | grep python
```

---

## ğŸ”’ NOTAS DE SEGURIDAD

- La app acepta conexiones externas (`0.0.0.0:5000`)
- Para producciÃ³n: usar Nginx + HTTPS
- Restringir Security Group a IPs especÃ­ficas
- Revisar logs periÃ³dicamente

---

**Desarrollado para testing con GuardTrail de Trend Micro** ğŸ”´âš«
